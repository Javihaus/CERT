{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure GPT-5 (or any model) Baseline in Google Colab\n",
    "\n",
    "This notebook measures complete baseline metrics for a new OpenAI model and adds it to the CERT registry.\n",
    "\n",
    "**What you'll measure:**\n",
    "- Behavioral Consistency (C): How stable is the model?\n",
    "- Performance Distribution (Œº, œÉ): Mean quality and variability\n",
    "- Context Propagation Effect (Œ≥): Benefit from sequential processing\n",
    "\n",
    "**Requirements:**\n",
    "- OpenAI API key\n",
    "- 10-15 minutes\n",
    "- ~$2-5 in API costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install CERT SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/Javihaus/CERT.git\n",
    "print(\"‚úì CERT SDK installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "\n",
    "import cert\n",
    "from cert.models import ModelRegistry\n",
    "from cert.providers import OpenAIProvider\n",
    "from cert.providers.base import ProviderConfig\n",
    "from cert.analysis.semantic import SemanticAnalyzer\n",
    "from cert.analysis.quality import QualityScorer\n",
    "from cert.core.metrics import (\n",
    "    behavioral_consistency,\n",
    "    empirical_performance_distribution,\n",
    "    coordination_effect,\n",
    ")\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "# Model to measure\n",
    "MODEL_NAME = \"gpt-4o\"  # Change to \"gpt-5\" or whatever model you're using\n",
    "MODEL_FAMILY = \"GPT-4o\"  # Change to \"GPT-5\" for display\n",
    "\n",
    "print(f\"‚úì Will measure baseline for: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analytical Prompts (from CERT paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the same prompts used in the paper for all baseline measurements\n",
    "ANALYTICAL_PROMPTS = [\n",
    "    \"Analyze the key factors in effective team communication.\",\n",
    "    \"Evaluate the main considerations for project risk management.\",\n",
    "    \"Assess the critical elements of successful strategic planning.\",\n",
    "    \"Identify the primary aspects of organizational change management.\",\n",
    "    \"Examine the essential components of decision-making frameworks.\",\n",
    "    \"Analyze the challenges in cross-functional collaboration.\",\n",
    "    \"Evaluate process optimization strategies in complex systems.\",\n",
    "    \"Assess quality assurance methodologies and their effectiveness.\",\n",
    "    \"Identify barriers to innovation in established organizations.\",\n",
    "    \"Examine factors influencing stakeholder engagement.\",\n",
    "    \"Analyze the relationship between leadership style and outcomes.\",\n",
    "    \"Evaluate resource allocation strategies in constrained environments.\",\n",
    "    \"Assess the impact of communication channels on information flow.\",\n",
    "    \"Identify success metrics for collaborative initiatives.\",\n",
    "    \"Examine the role of feedback mechanisms in performance improvement.\",\n",
    "]\n",
    "\n",
    "print(f\"‚úì Using {len(ANALYTICAL_PROMPTS)} analytical prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ProviderConfig(\n",
    "    api_key=api_key,\n",
    "    model_name=MODEL_NAME,\n",
    "    temperature=0.7,  # Paper uses 0.7\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "provider = OpenAIProvider(config)\n",
    "\n",
    "print(f\"‚úì Provider initialized for {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. STEP 1: Measure Behavioral Consistency (C)\n",
    "\n",
    "Generate 20 responses to the same prompt and measure consistency using semantic distance variability.\n",
    "\n",
    "**Formula:** C = 1 - (Std[distances] / Mean[distances])\n",
    "\n",
    "**Time:** ~2-3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_consistency(provider, prompt, n_trials=20):\n",
    "    \"\"\"Measure Behavioral Consistency C from Equation 1.\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"MEASURING BEHAVIORAL CONSISTENCY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Prompt: {prompt[:60]}...\")\n",
    "    print(f\"Trials: {n_trials}\\n\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        responses.append(response)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Progress: {i+1}/{n_trials} responses generated\")\n",
    "    \n",
    "    print(\"\\n  Calculating semantic distances...\")\n",
    "    analyzer = SemanticAnalyzer()\n",
    "    distances = analyzer.pairwise_distances(responses)\n",
    "    \n",
    "    consistency = behavioral_consistency(distances)\n",
    "    \n",
    "    print(f\"\\n  ‚úì Behavioral Consistency: C = {consistency:.3f}\")\n",
    "    print(f\"    Mean distance: {np.mean(distances):.3f}\")\n",
    "    print(f\"    Std distance:  {np.std(distances):.3f}\")\n",
    "    \n",
    "    return consistency\n",
    "\n",
    "# Run measurement\n",
    "consistency = await measure_consistency(\n",
    "    provider=provider,\n",
    "    prompt=ANALYTICAL_PROMPTS[0],\n",
    "    n_trials=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. STEP 2: Measure Performance Distribution (Œº, œÉ)\n",
    "\n",
    "Generate responses to 15 different prompts and score quality.\n",
    "\n",
    "**Formula:** Œº = Mean[quality_scores], œÉ = Std[quality_scores]\n",
    "\n",
    "**Time:** ~3-5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_performance(provider, prompts):\n",
    "    \"\"\"Measure Performance Distribution (Œº, œÉ) from Equation 2.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MEASURING PERFORMANCE DISTRIBUTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Prompts: {len(prompts)}\\n\")\n",
    "    \n",
    "    scorer = QualityScorer()\n",
    "    quality_scores = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        components = scorer.score(prompt, response)\n",
    "        quality_scores.append(components.composite_score)\n",
    "        \n",
    "        print(f\"  Prompt {i+1}/{len(prompts)}: Q = {components.composite_score:.3f}\")\n",
    "    \n",
    "    mu, sigma = empirical_performance_distribution(np.array(quality_scores))\n",
    "    \n",
    "    print(f\"\\n  ‚úì Performance Distribution:\")\n",
    "    print(f\"    Mean (Œº):     {mu:.3f}\")\n",
    "    print(f\"    Std Dev (œÉ):  {sigma:.3f}\")\n",
    "    print(f\"    Min quality:  {min(quality_scores):.3f}\")\n",
    "    print(f\"    Max quality:  {max(quality_scores):.3f}\")\n",
    "    \n",
    "    return mu, sigma, quality_scores\n",
    "\n",
    "# Run measurement\n",
    "mu, sigma, quality_scores = await measure_performance(\n",
    "    provider=provider,\n",
    "    prompts=ANALYTICAL_PROMPTS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. STEP 3: Measure Context Propagation Effect (Œ≥) for 2-agent pipelines\n",
    "\n",
    "Simulate 2-agent sequential processing and compare to independent execution.\n",
    "\n",
    "**Formula:** Œ≥ = P_sequential / (P_agent1 √ó P_agent2)\n",
    "\n",
    "**Time:** ~5-7 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_coordination_2agent(provider, prompts, n_pairs=5):\n",
    "    \"\"\"Measure Context Propagation Effect Œ≥ for 2-agent pipelines.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MEASURING CONTEXT PROPAGATION EFFECT (2-agent)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Testing {n_pairs} prompt pairs\\n\")\n",
    "    \n",
    "    scorer = QualityScorer()\n",
    "    gamma_values = []\n",
    "    \n",
    "    for i in range(n_pairs):\n",
    "        prompt1 = prompts[i * 2]\n",
    "        prompt2 = prompts[i * 2 + 1]\n",
    "        \n",
    "        print(f\"\\n  Pair {i+1}/{n_pairs}:\")\n",
    "        print(f\"    Prompt 1: {prompt1[:50]}...\")\n",
    "        print(f\"    Prompt 2: {prompt2[:50]}...\")\n",
    "        \n",
    "        # Independent execution\n",
    "        response1_indep = await provider.generate_response(prompt=prompt1, temperature=0.7)\n",
    "        response2_indep = await provider.generate_response(prompt=prompt2, temperature=0.7)\n",
    "        \n",
    "        quality1 = scorer.score(prompt1, response1_indep).composite_score\n",
    "        quality2 = scorer.score(prompt2, response2_indep).composite_score\n",
    "        \n",
    "        print(f\"      Independent Q1: {quality1:.3f}, Q2: {quality2:.3f}\")\n",
    "        \n",
    "        # Sequential execution (agent 2 sees agent 1's output)\n",
    "        response1_seq = await provider.generate_response(prompt=prompt1, temperature=0.7)\n",
    "        \n",
    "        # Agent 2 prompt includes agent 1's output as context\n",
    "        prompt2_with_context = f\"Building on this analysis:\\n\\n{response1_seq}\\n\\nNow: {prompt2}\"\n",
    "        response2_seq = await provider.generate_response(prompt=prompt2_with_context, temperature=0.7)\n",
    "        \n",
    "        quality_seq = scorer.score(prompt2, response2_seq).composite_score\n",
    "        \n",
    "        print(f\"      Sequential Q:   {quality_seq:.3f}\")\n",
    "        \n",
    "        # Calculate Œ≥ (Equation 3)\n",
    "        gamma = coordination_effect(\n",
    "            coordinated_performance=quality_seq,\n",
    "            independent_performances=[quality1, quality2]\n",
    "        )\n",
    "        \n",
    "        gamma_values.append(gamma)\n",
    "        print(f\"      Œ≥ = {gamma:.3f}\")\n",
    "    \n",
    "    mean_gamma = np.mean(gamma_values)\n",
    "    \n",
    "    print(f\"\\n  ‚úì Context Propagation Effect (2-agent):\")\n",
    "    print(f\"    Mean Œ≥:  {mean_gamma:.3f}\")\n",
    "    print(f\"    Std Œ≥:   {np.std(gamma_values):.3f}\")\n",
    "    print(f\"    Min Œ≥:   {min(gamma_values):.3f}\")\n",
    "    print(f\"    Max Œ≥:   {max(gamma_values):.3f}\")\n",
    "    \n",
    "    return mean_gamma\n",
    "\n",
    "# Run measurement\n",
    "gamma_2agent = await measure_coordination_2agent(\n",
    "    provider=provider,\n",
    "    prompts=ANALYTICAL_PROMPTS,\n",
    "    n_pairs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEASUREMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_NAME} ({MODEL_FAMILY})\")\n",
    "print(f\"\\nBaseline Metrics:\")\n",
    "print(f\"  Consistency (C):            {consistency:.3f}\")\n",
    "print(f\"  Mean Performance (Œº):       {mu:.3f}\")\n",
    "print(f\"  Std Performance (œÉ):        {sigma:.3f}\")\n",
    "print(f\"  Context Effect Œ≥ (2-agent): {gamma_2agent:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison to Paper Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paper baselines\n",
    "paper_data = {\n",
    "    \"Model\": [\"Claude 3 Haiku\", \"GPT-4o\", \"Grok 3\", \"Gemini 3.5 Pro\", f\"{MODEL_FAMILY} (NEW)\"],\n",
    "    \"C\": [0.831, 0.831, 0.863, 0.895, consistency],\n",
    "    \"Œº\": [0.595, 0.638, 0.658, 0.831, mu],\n",
    "    \"Œ≥\": [1.462, 1.562, 1.625, 1.137, gamma_2agent],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(paper_data)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON TO PAPER BASELINES\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INTERPRETATION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if consistency > 0.85:\n",
    "    print(f\"\\n‚úì HIGH consistency (C={consistency:.3f})\")\n",
    "    print(\"  Very stable, predictable behavior. Low monitoring overhead.\")\n",
    "elif consistency > 0.80:\n",
    "    print(f\"\\n‚úì GOOD consistency (C={consistency:.3f})\")\n",
    "    print(\"  Stable behavior with minor variance. Standard monitoring.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† MODERATE consistency (C={consistency:.3f})\")\n",
    "    print(\"  Higher variance. Needs enhanced monitoring.\")\n",
    "\n",
    "if mu > 0.70:\n",
    "    print(f\"\\n‚úì HIGH baseline performance (Œº={mu:.3f})\")\n",
    "    print(\"  Strong individual task performance.\")\n",
    "elif mu > 0.60:\n",
    "    print(f\"\\n‚úì GOOD baseline performance (Œº={mu:.3f})\")\n",
    "    print(\"  Solid performance.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† MODERATE baseline performance (Œº={mu:.3f})\")\n",
    "    print(\"  Room for improvement.\")\n",
    "\n",
    "if gamma_2agent > 1.5:\n",
    "    print(f\"\\n‚úì STRONG context propagation (Œ≥={gamma_2agent:.3f})\")\n",
    "    print(\"  Benefits greatly from sequential processing. Excellent for multi-agent pipelines.\")\n",
    "elif gamma_2agent > 1.2:\n",
    "    print(f\"\\n‚úì MODERATE context propagation (Œ≥={gamma_2agent:.3f})\")\n",
    "    print(\"  Benefits from context accumulation.\")\n",
    "elif gamma_2agent > 1.0:\n",
    "    print(f\"\\n‚ö† WEAK context propagation (Œ≥={gamma_2agent:.3f})\")\n",
    "    print(\"  Minimal benefit from sequential processing. May work better independently.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† NEGATIVE context propagation (Œ≥={gamma_2agent:.3f})\")\n",
    "    print(\"  Context may be degrading performance. Avoid sequential pipelines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Register in ModelRegistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the new baseline\n",
    "baseline = ModelRegistry.register_custom_baseline(\n",
    "    model_id=MODEL_NAME,\n",
    "    provider=\"openai\",\n",
    "    model_family=MODEL_FAMILY,\n",
    "    consistency=consistency,\n",
    "    mean_performance=mu,\n",
    "    std_performance=sigma,\n",
    "    coordination_2agent=gamma_2agent,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Registered: {baseline}\")\n",
    "print(f\"\\nYou can now use this baseline:\")\n",
    "print(f\"  baseline = ModelRegistry.get_model('{MODEL_NAME}')\")\n",
    "print(f\"  print(baseline.consistency)  # {consistency:.3f}\")\n",
    "print(f\"  print(baseline.coordination_2agent)  # {gamma_2agent:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Code to Add Permanently\n",
    "\n",
    "To add this model permanently to the SDK, copy this code into `src/cert/models.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TO ADD PERMANENTLY TO src/cert/models.py\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAdd this to the _VALIDATED_MODELS dictionary:\\n\")\n",
    "\n",
    "code = f'''    # {MODEL_FAMILY}\n",
    "    \"{MODEL_NAME}\": ModelBaseline(\n",
    "        model_id=\"{MODEL_NAME}\",\n",
    "        provider=\"openai\",\n",
    "        model_family=\"{MODEL_FAMILY}\",\n",
    "        consistency={consistency:.3f},\n",
    "        mean_performance={mu:.3f},\n",
    "        std_performance={sigma:.3f},\n",
    "        coordination_2agent={gamma_2agent:.3f},\n",
    "        coordination_5agent=None,  # Measure separately if needed\n",
    "        paper_section=\"Custom Measurement\",\n",
    "        validation_date=\"{cert.__version__}\",\n",
    "    ),'''\n",
    "\n",
    "print(code)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEASUREMENT COMPLETE! üéâ\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test the New Baseline\n",
    "\n",
    "Let's verify the baseline is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the baseline\n",
    "retrieved_baseline = ModelRegistry.get_model(MODEL_NAME)\n",
    "\n",
    "if retrieved_baseline:\n",
    "    print(\"‚úì Baseline successfully registered and retrieved!\")\n",
    "    print(f\"\\n{retrieved_baseline}\")\n",
    "    print(f\"\\nŒ≥ (2-agent): {retrieved_baseline.coordination_2agent:.3f}\")\n",
    "    \n",
    "    # Compare to your measured Œ≥ = 3.632 from the 3-agent pipeline\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"EXPECTED vs MEASURED Œ≥\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nBaseline Œ≥ (2-agent): {retrieved_baseline.coordination_2agent:.3f}\")\n",
    "    print(f\"Your pipeline Œ≥ (3-agent): 3.632\")\n",
    "    print(f\"\\nDifference: The 3-agent pipeline shows STRONGER context propagation\")\n",
    "    print(f\"because each agent builds on accumulated context from MULTIPLE\")\n",
    "    print(f\"previous agents, creating a compounding effect.\")\n",
    "else:\n",
    "    print(\"‚ùå Error: Baseline not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
