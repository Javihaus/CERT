{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT SDK - Basic Usage with Validated Models\n",
    "\n",
    "This notebook demonstrates the recommended workflow for using CERT SDK:\n",
    "\n",
    "1. **Select a validated model** from the registry\n",
    "2. **Initialize provider** with your API key\n",
    "3. **Measure behavioral consistency** to check agent predictability\n",
    "4. **Measure performance distribution** to understand output quality\n",
    "5. **Compare results** to validated baselines from the paper\n",
    "\n",
    "**Estimated time:** 2-3 minutes\n",
    "\n",
    "**What you'll need:**\n",
    "- API key for one of the validated models (OpenAI, Google, xAI, or Anthropic)\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CERT SDK if not already installed\n# !pip install cert-sdk\n\nimport asyncio\nimport cert\nfrom cert.models import ModelRegistry\nfrom cert.providers import OpenAIProvider, GoogleProvider, XAIProvider\nfrom cert.providers.base import ProviderConfig\nfrom cert.analysis.semantic import SemanticAnalyzer\nfrom cert.analysis.quality import QualityScorer\nfrom cert.core.metrics import (\n    behavioral_consistency,\n    empirical_performance_distribution,\n)\nimport numpy as np"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Browse Available Validated Models\n",
    "\n",
    "CERT SDK includes pre-validated baselines from the paper for these models. Let's see what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the convenient utils function to display all validated models\ncert.print_models()\n\n# You can also filter by provider:\n# cert.print_models(provider=\"openai\")\n\n# Or get detailed info about a specific model:\n# cert.get_model_info(\"gpt-4o\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Select Your Model and Configure Provider\n",
    "\n",
    "Choose a model you have API access to and configure the provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model from the registry\n",
    "# Options: \"gpt-4o\", \"gpt-4o-mini\", \"grok-3\", \"gemini-3.5-pro\", \"claude-3-5-haiku-20241022\"\n",
    "selected_model_id = \"gpt-4o\"  # Change this to your model\n",
    "\n",
    "# Get baseline from registry\n",
    "model_baseline = ModelRegistry.get_model(selected_model_id)\n",
    "\n",
    "print(f\"✓ Selected: {model_baseline.model_family} ({model_baseline.model_id})\")\n",
    "print(f\"  Using validated baseline from paper:\")\n",
    "print(f\"  C={model_baseline.consistency:.3f}, μ={model_baseline.mean_performance:.3f}, σ={model_baseline.std_performance:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your API key (it will be hidden in Jupyter)\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(f\"Enter your {model_baseline.provider} API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize provider\n",
    "config = ProviderConfig(\n",
    "    api_key=api_key,\n",
    "    model_name=model_baseline.model_id,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# Map provider name to class\n",
    "provider_map = {\n",
    "    \"openai\": OpenAIProvider,\n",
    "    \"google\": GoogleProvider,\n",
    "    \"xai\": XAIProvider,\n",
    "}\n",
    "\n",
    "ProviderClass = provider_map[model_baseline.provider]\n",
    "provider = ProviderClass(config)\n",
    "\n",
    "print(f\"✓ Provider initialized: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Measure Behavioral Consistency\n",
    "\n",
    "**What is it?** Behavioral consistency measures how predictable your agent is when given the same prompt multiple times.\n",
    "\n",
    "**Why it matters:** Inconsistent agents are unpredictable in production.\n",
    "\n",
    "**How it works:** We generate multiple responses to the same prompt and measure semantic distances between them. Lower distance = higher consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_consistency(provider, model_baseline, n_trials=10):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Measuring Behavioral Consistency\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Standard prompt from paper's experiments\n",
    "    prompt = \"Analyze the key factors in effective business strategy implementation.\"\n",
    "    \n",
    "    print(f\"\\nGenerating {n_trials} responses to measure consistency...\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    \n",
    "    # Generate responses\n",
    "    responses = []\n",
    "    for i in range(n_trials):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        responses.append(response)\n",
    "        print(f\"  Response {i+1}/{n_trials} generated ({len(response)} chars)\")\n",
    "    \n",
    "    # Calculate semantic distances\n",
    "    print(f\"\\nCalculating semantic distances...\")\n",
    "    analyzer = SemanticAnalyzer()\n",
    "    distances = analyzer.pairwise_distances(responses)\n",
    "    \n",
    "    # Calculate consistency\n",
    "    consistency = behavioral_consistency(distances)\n",
    "    \n",
    "    # Compare to paper baseline\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Measured Consistency: C = {consistency:.3f}\")\n",
    "    print(f\"Paper Baseline:       C = {model_baseline.consistency:.3f}\")\n",
    "    \n",
    "    diff = consistency - model_baseline.consistency\n",
    "    if abs(diff) < 0.05:\n",
    "        status = \"✓ Within expected range\"\n",
    "    elif diff > 0:\n",
    "        status = \"↑ Higher than baseline (more consistent)\"\n",
    "    else:\n",
    "        status = \"↓ Lower than baseline (less consistent)\"\n",
    "    \n",
    "    print(f\"Difference:           {diff:+.3f} ({status})\")\n",
    "    \n",
    "    return consistency\n",
    "\n",
    "# Run the measurement\n",
    "consistency = await measure_consistency(provider, model_baseline, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Measure Performance Distribution\n",
    "\n",
    "**What is it?** Performance distribution measures the quality of your agent's outputs across different prompts.\n",
    "\n",
    "**Why it matters:** Understanding mean (μ) and variability (σ) helps predict production behavior.\n",
    "\n",
    "**How it works:** We use multidimensional quality scoring (semantic relevance, linguistic coherence, content density) across multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_performance(provider, model_baseline, n_trials=5):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Measuring Performance Distribution\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Standard prompts from paper's experiments\n",
    "    prompts = [\n",
    "        \"Analyze the key factors in business strategy.\",\n",
    "        \"Evaluate the main considerations for project management.\",\n",
    "        \"Assess the critical elements in organizational change.\",\n",
    "        \"Identify the primary aspects of market analysis.\",\n",
    "        \"Examine the essential components of risk assessment.\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nGenerating responses for {n_trials} prompts...\")\n",
    "    \n",
    "    # Generate and score responses\n",
    "    scorer = QualityScorer()\n",
    "    quality_scores = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts[:n_trials]):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        # Score using paper's quality metrics\n",
    "        components = scorer.score(prompt, response)\n",
    "        quality_scores.append(components.composite_score)\n",
    "        \n",
    "        print(f\"  Prompt {i+1}: Q = {components.composite_score:.3f}\")\n",
    "        print(f\"    (semantic: {components.semantic_relevance:.3f}, \"\n",
    "              f\"coherence: {components.linguistic_coherence:.3f}, \"\n",
    "              f\"density: {components.content_density:.3f})\")\n",
    "    \n",
    "    # Calculate distribution\n",
    "    mu, sigma = empirical_performance_distribution(np.array(quality_scores))\n",
    "    \n",
    "    # Compare to paper baseline\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Results:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Measured Performance: μ = {mu:.3f}, σ = {sigma:.3f}\")\n",
    "    print(f\"Paper Baseline:       μ = {model_baseline.mean_performance:.3f}, σ = {model_baseline.std_performance:.3f}\")\n",
    "    \n",
    "    mu_diff = mu - model_baseline.mean_performance\n",
    "    print(f\"Mean difference:      {mu_diff:+.3f}\")\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "# Run the measurement\n",
    "mu, sigma = await measure_performance(provider, model_baseline, n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Coordination Effect Prediction\n",
    "\n",
    "**What is it?** Coordination effect (γ) predicts how much agents improve when working together vs independently.\n",
    "\n",
    "**Why it matters:** Tells you if adding more agents actually helps or just adds latency.\n",
    "\n",
    "**How to interpret:**\n",
    "- γ > 1: Agents coordinate well (synergy)\n",
    "- γ = 1: No benefit from coordination\n",
    "- γ < 1: Agents interfere with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Coordination Effect Prediction (from Paper)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    print(f\"\\nValidated 2-agent coordination effect from paper:\")\n",
    "    print(f\"  γ = {model_baseline.coordination_2agent:.3f}\")\n",
    "    \n",
    "    # Calculate expected coordination performance\n",
    "    independent_perf = model_baseline.mean_performance\n",
    "    coordinated_perf = independent_perf * independent_perf * model_baseline.coordination_2agent\n",
    "    \n",
    "    print(f\"\\nPrediction for 2-agent sequential pipeline:\")\n",
    "    print(f\"  Independent performance: {independent_perf:.3f}\")\n",
    "    print(f\"  Expected coordinated:    {coordinated_perf:.3f}\")\n",
    "    print(f\"  Improvement:             {(coordinated_perf/independent_perf - 1)*100:+.1f}%\")\n",
    "else:\n",
    "    print(\"\\n⚠ 2-agent coordination baseline not available for this model.\")\n",
    "    print(\"  You can measure it using coordination experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's a complete comparison of your measurements vs the paper baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nModel: {model_baseline.model_family} ({model_baseline.model_id})\")\n",
    "print(f\"\\nYour Measurements:\")\n",
    "print(f\"  Consistency:   C = {consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {mu:.3f}, σ = {sigma:.3f}\")\n",
    "print(f\"\\nPaper Baselines:\")\n",
    "print(f\"  Consistency:   C = {model_baseline.consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {model_baseline.mean_performance:.3f}, σ = {model_baseline.std_performance:.3f}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    print(f\"  2-agent γ:     {model_baseline.coordination_2agent:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Basic measurements complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Run more trials for statistical significance (20+ recommended)\")\n",
    "print(\"  - Measure coordination effects with multi-agent pipelines\")\n",
    "print(\"  - See advanced_usage.ipynb for custom models and domain-specific tasks\")\n",
    "print(\"  - See examples/two_agent_coordination.ipynb for coordination measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Your Results\n",
    "\n",
    "### Behavioral Consistency (C)\n",
    "- **C > 0.85**: Highly consistent - safe for production\n",
    "- **0.7 < C < 0.85**: Moderately consistent - acceptable with monitoring\n",
    "- **C < 0.7**: Inconsistent - needs prompt engineering or different model\n",
    "\n",
    "### Performance (μ, σ)\n",
    "- **μ**: Higher mean = better quality outputs\n",
    "- **σ**: Lower std dev = more predictable quality\n",
    "\n",
    "### What if my results differ from baseline?\n",
    "- Small differences (±0.05) are normal due to sampling\n",
    "- Larger differences may indicate:\n",
    "  - Different prompt distributions\n",
    "  - Model version changes\n",
    "  - Temperature/parameter differences\n",
    "  - Domain-specific behavior\n",
    "\n",
    "### Production Recommendations\n",
    "1. **High consistency + High performance**: Deploy with confidence\n",
    "2. **High consistency + Low performance**: Consider prompt engineering\n",
    "3. **Low consistency**: Investigate before production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}