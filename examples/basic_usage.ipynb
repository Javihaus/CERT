{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT SDK - Basic Usage with Validated Models\n",
    "\n",
    "This notebook demonstrates the recommended workflow for using CERT SDK:\n",
    "\n",
    "1. **Select a validated model** from the registry\n",
    "2. **Initialize provider** with your API key\n",
    "3. **Measure behavioral consistency** to check agent predictability\n",
    "4. **Measure performance distribution** to understand output quality\n",
    "5. **Compare results** to validated baselines from the paper\n",
    "\n",
    "**Estimated time:** 2-3 minutes\n",
    "\n",
    "**What you'll need:**\n",
    "- API key for one of the validated models (OpenAI, Google, xAI, or Anthropic)\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CERT SDK if not already installed\n# !pip install cert-sdk\n\nimport asyncio\nimport cert"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Browse Available Validated Models\n",
    "\n",
    "CERT SDK includes pre-validated baselines from the paper for these models. Let's see what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the convenient utils function to display all validated models\ncert.print_models()\n\n# You can also filter by provider:\n# cert.print_models(provider=\"openai\")\n\n# Or get detailed info about a specific model:\n# cert.get_model_info(\"gpt-4o\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Select Your Model and Configure Provider\n",
    "\n",
    "Choose a model you have API access to and configure the provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select a model from the registry\n# Options: \"gpt-4o\", \"gpt-4o-mini\", \"grok-3\", \"gemini-3.5-pro\", \"claude-3-5-haiku-20241022\"\nselected_model_id = \"gpt-4o\"  # Change this to your model\n\n# Get baseline from registry\nmodel_baseline = cert.ModelRegistry.get_model(selected_model_id)\n\nprint(f\"✓ Selected: {model_baseline.model_family} ({model_baseline.model_id})\")\nprint(f\"  Using validated baseline from paper:\")\nprint(f\"  C={model_baseline.consistency:.3f}, μ={model_baseline.mean_performance:.3f}, σ={model_baseline.std_performance:.3f}\")\nprint()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enter your API key (it will be hidden in Jupyter)\nfrom getpass import getpass\n\napi_key = getpass(\"Enter your API key: \")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize provider - simple and direct!\nprovider = cert.create_provider(\n    api_key=api_key,\n    model_name=selected_model_id,\n    temperature=0.7,\n    max_tokens=1024,\n)\n\nprint(f\"✓ Provider initialized: {provider}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Measure Behavioral Consistency\n",
    "\n",
    "**What is it?** Behavioral consistency measures how predictable your agent is when given the same prompt multiple times.\n",
    "\n",
    "**Why it matters:** Inconsistent agents are unpredictable in production.\n",
    "\n",
    "**How it works:** We generate multiple responses to the same prompt and measure semantic distances between them. Lower distance = higher consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Measure consistency - simple one-line call!\nconsistency = await cert.measure_consistency(\n    provider=provider,\n    n_trials=10,\n    baseline=model_baseline,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Measure Performance Distribution\n",
    "\n",
    "**What is it?** Performance distribution measures the quality of your agent's outputs across different prompts.\n",
    "\n",
    "**Why it matters:** Understanding mean (μ) and variability (σ) helps predict production behavior.\n",
    "\n",
    "**How it works:** We use multidimensional quality scoring (semantic relevance, linguistic coherence, content density) across multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Measure performance - simple one-line call!\nmu, sigma = await cert.measure_performance(\n    provider=provider,\n    baseline=model_baseline,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Coordination Effect Prediction\n",
    "\n",
    "**What is it?** Coordination effect (γ) predicts how much agents improve when working together vs independently.\n",
    "\n",
    "**Why it matters:** Tells you if adding more agents actually helps or just adds latency.\n",
    "\n",
    "**How to interpret:**\n",
    "- γ > 1: Agents coordinate well (synergy)\n",
    "- γ = 1: No benefit from coordination\n",
    "- γ < 1: Agents interfere with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Coordination Effect Prediction (from Paper)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    print(f\"\\nValidated 2-agent coordination effect from paper:\")\n",
    "    print(f\"  γ = {model_baseline.coordination_2agent:.3f}\")\n",
    "    \n",
    "    # Calculate expected coordination performance\n",
    "    independent_perf = model_baseline.mean_performance\n",
    "    coordinated_perf = independent_perf * independent_perf * model_baseline.coordination_2agent\n",
    "    \n",
    "    print(f\"\\nPrediction for 2-agent sequential pipeline:\")\n",
    "    print(f\"  Independent performance: {independent_perf:.3f}\")\n",
    "    print(f\"  Expected coordinated:    {coordinated_perf:.3f}\")\n",
    "    print(f\"  Improvement:             {(coordinated_perf/independent_perf - 1)*100:+.1f}%\")\n",
    "else:\n",
    "    print(\"\\n⚠ 2-agent coordination baseline not available for this model.\")\n",
    "    print(\"  You can measure it using coordination experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's a complete comparison of your measurements vs the paper baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nModel: {model_baseline.model_family} ({model_baseline.model_id})\")\n",
    "print(f\"\\nYour Measurements:\")\n",
    "print(f\"  Consistency:   C = {consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {mu:.3f}, σ = {sigma:.3f}\")\n",
    "print(f\"\\nPaper Baselines:\")\n",
    "print(f\"  Consistency:   C = {model_baseline.consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {model_baseline.mean_performance:.3f}, σ = {model_baseline.std_performance:.3f}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    print(f\"  2-agent γ:     {model_baseline.coordination_2agent:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Basic measurements complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Run more trials for statistical significance (20+ recommended)\")\n",
    "print(\"  - Measure coordination effects with multi-agent pipelines\")\n",
    "print(\"  - See advanced_usage.ipynb for custom models and domain-specific tasks\")\n",
    "print(\"  - See examples/two_agent_coordination.ipynb for coordination measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Your Results\n",
    "\n",
    "### Behavioral Consistency (C)\n",
    "- **C > 0.85**: Highly consistent - safe for production\n",
    "- **0.7 < C < 0.85**: Moderately consistent - acceptable with monitoring\n",
    "- **C < 0.7**: Inconsistent - needs prompt engineering or different model\n",
    "\n",
    "### Performance (μ, σ)\n",
    "- **μ**: Higher mean = better quality outputs\n",
    "- **σ**: Lower std dev = more predictable quality\n",
    "\n",
    "### What if my results differ from baseline?\n",
    "- Small differences (±0.05) are normal due to sampling\n",
    "- Larger differences may indicate:\n",
    "  - Different prompt distributions\n",
    "  - Model version changes\n",
    "  - Temperature/parameter differences\n",
    "  - Domain-specific behavior\n",
    "\n",
    "### Production Recommendations\n",
    "1. **High consistency + High performance**: Deploy with confidence\n",
    "2. **High consistency + Low performance**: Consider prompt engineering\n",
    "3. **Low consistency**: Investigate before production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}