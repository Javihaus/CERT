{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT SDK - Basic Usage with Validated Models\n",
    "\n",
    "This notebook demonstrates the recommended workflow for using CERT SDK:\n",
    "\n",
    "1. **Select a validated model** from the registry\n",
    "2. **Initialize provider** with your API key\n",
    "3. **Measure behavioral consistency** to check output predictability\n",
    "4. **Measure performance distribution** to understand output quality\n",
    "5. **Compare results** to validated baselines from the paper\n",
    "\n",
    "**Estimated time:** 2-3 minutes\n",
    "\n",
    "**What you'll need:**\n",
    "- API key for one of the validated models (OpenAI, Google, xAI, or Anthropic)\n",
    "- Python 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CERT SDK\n# Option 1: From PyPI (when available)\n# !pip install cert-sdk\n\n# Option 2: Directly from GitHub repository (development version)\n# !pip install git+https://github.com/Javihaus/CERT.git\n\nimport asyncio\nimport cert"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Browse Available Validated Models\n",
    "\n",
    "CERT SDK includes pre-validated baselines from the paper for these models. Let's see what's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the convenient utils function to display all validated models\n",
    "cert.print_models()\n",
    "\n",
    "# You can also filter by provider:\n",
    "# cert.print_models(provider=\"openai\")\n",
    "\n",
    "# Or get detailed info about a specific model:\n",
    "# cert.get_model_info(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Select Your Model and Configure Provider\n",
    "\n",
    "Choose a model you have API access to and configure the provider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model from the registry\n",
    "# Options: \"gpt-4o\", \"gpt-4o-mini\", \"grok-3\", \"gemini-3.5-pro\", \"claude-3-5-haiku-20241022\"\n",
    "selected_model_id = \"gpt-4o\"  # Change this to your model\n",
    "\n",
    "# Get baseline from registry\n",
    "model_baseline = cert.ModelRegistry.get_model(selected_model_id)\n",
    "\n",
    "print(f\"✓ Selected: {model_baseline.model_family} ({model_baseline.model_id})\")\n",
    "print(f\"  Using validated baseline from paper:\")\n",
    "print(f\"  C={model_baseline.consistency:.3f}, μ={model_baseline.mean_performance:.3f}, σ={model_baseline.std_performance:.3f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your API key (it will be hidden in Jupyter)\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize provider - simple and direct!\n",
    "provider = cert.create_provider(\n",
    "    api_key=api_key,\n",
    "    model_name=selected_model_id,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(f\"✓ Provider initialized: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Measure Behavioral Consistency\n",
    "\n",
    "**What is it?** Behavioral consistency measures how predictable a model is when given the same prompt multiple times.\n",
    "\n",
    "**Why it matters:** Inconsistent models are unpredictable in production.\n",
    "\n",
    "**How it works:** We generate multiple responses to the same prompt and measure semantic distances between them. Lower distance variance = higher consistency.\n",
    "\n",
    "**Formula:** `C = 1 - (σ(d) / μ(d))` where d = semantic distances between responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure consistency - simple one-line call!\n",
    "consistency = await cert.measure_consistency(\n",
    "    provider=provider,\n",
    "    n_trials=10,\n",
    "    baseline=model_baseline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Measure Performance Distribution\n",
    "\n",
    "**What is it?** Performance distribution measures the quality of model outputs across different prompts.\n",
    "\n",
    "**Why it matters:** Understanding mean (μ) and variability (σ) helps predict production behavior.\n",
    "\n",
    "**How it works:** We use multidimensional quality scoring (semantic relevance, linguistic coherence, content density) across multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance - simple one-line call!\n",
    "mu, sigma = await cert.measure_performance(\n",
    "    provider=provider,\n",
    "    baseline=model_baseline,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Context Propagation Effect Prediction\n",
    "\n",
    "**What is it?** Context propagation effect (γ) measures how performance changes when models process accumulated context in sequential pipelines.\n",
    "\n",
    "**What it measures:** When a later model sees the output of an earlier model, does quality improve due to extended context?\n",
    "\n",
    "**How to interpret:**\n",
    "- γ > 1: Sequential context accumulation improves performance\n",
    "- γ = 1: No benefit from accumulated context\n",
    "- γ < 1: Context accumulation degrades performance (attention dilution, context window issues)\n",
    "\n",
    "**What this does NOT measure:**\n",
    "- ❌ Not measuring \"agent intelligence\" or \"coordination principles\"\n",
    "- ❌ Not detecting genuine collaboration or planning\n",
    "- ❌ Not explaining WHY context helps (black box measurement)\n",
    "\n",
    "**What it IS:**\n",
    "- ✅ Statistical characterization of attention mechanism behavior\n",
    "- ✅ Operational metric for pipeline architecture decisions\n",
    "- ✅ Engineering measurement: \"which configurations show reliable patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Context Propagation Effect Prediction (from Paper)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    gamma = model_baseline.coordination_2agent\n",
    "    print(f\"\\nValidated 2-model sequential context effect from paper:\")\n",
    "    print(f\"  γ = {gamma:.3f}\")\n",
    "    \n",
    "    # Calculate expected sequential performance\n",
    "    independent_perf = model_baseline.mean_performance\n",
    "    sequential_perf = independent_perf * independent_perf * gamma\n",
    "    \n",
    "    print(f\"\\nPrediction for 2-model sequential pipeline:\")\n",
    "    print(f\"  Independent performance: {independent_perf:.3f}\")\n",
    "    print(f\"  Expected sequential:     {sequential_perf:.3f}\")\n",
    "    print(f\"  Improvement:             {(sequential_perf/independent_perf - 1)*100:+.1f}%\")\n",
    "    \n",
    "    print(f\"\\nOperational Interpretation:\")\n",
    "    if gamma > 1.2:\n",
    "        print(f\"  → Strong context propagation benefit\")\n",
    "        print(f\"  → Sequential processing improves quality substantially\")\n",
    "    elif gamma > 1.0:\n",
    "        print(f\"  → Moderate context propagation benefit\")\n",
    "        print(f\"  → Sequential processing helps but gains are modest\")\n",
    "    else:\n",
    "        print(f\"  → Context accumulation does not improve performance\")\n",
    "        print(f\"  → Consider single-model or parallel architectures\")\nelse:\n",
    "    print(\"\\n⚠ 2-model context effect baseline not available for this model.\")\n",
    "    print(\"  You can measure it using sequential pipeline experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here's a complete comparison of your measurements vs the paper baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Summary\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nModel: {model_baseline.model_family} ({model_baseline.model_id})\")\n",
    "print(f\"\\nYour Measurements:\")\n",
    "print(f\"  Consistency:   C = {consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {mu:.3f}, σ = {sigma:.3f}\")\n",
    "print(f\"\\nPaper Baselines:\")\n",
    "print(f\"  Consistency:   C = {model_baseline.consistency:.3f}\")\n",
    "print(f\"  Performance:   μ = {model_baseline.mean_performance:.3f}, σ = {model_baseline.std_performance:.3f}\")\n",
    "\n",
    "if model_baseline.coordination_2agent:\n",
    "    print(f\"  2-model γ:     {model_baseline.coordination_2agent:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓ Basic measurements complete!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  - Run more trials for statistical significance (20+ recommended)\")\n",
    "print(\"  - Measure context effects with sequential pipelines\")\n",
    "print(\"  - See advanced_usage.ipynb for custom models and domain-specific tasks\")\n",
    "print(\"  - See langchain_research_writer_pipeline.ipynb for real pipeline example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Your Results\n",
    "\n",
    "### Behavioral Consistency (C)\n",
    "- **C > 0.85**: Highly consistent - safe for production\n",
    "- **0.7 < C < 0.85**: Moderately consistent - acceptable with monitoring\n",
    "- **C < 0.7**: Inconsistent - needs prompt engineering or different model\n",
    "\n",
    "### Performance (μ, σ)\n",
    "- **μ**: Higher mean = better quality outputs\n",
    "- **σ**: Lower std dev = more predictable quality\n",
    "\n",
    "### Context Propagation Effect (γ)\n",
    "- **γ > 1.2**: Strong benefit from sequential processing\n",
    "- **1.0 < γ < 1.2**: Moderate benefit\n",
    "- **γ < 1.0**: Context accumulation degrades performance\n",
    "\n",
    "### What if my results differ from baseline?\n",
    "- Small differences (±0.05) are normal due to sampling\n",
    "- Larger differences may indicate:\n",
    "  - Different prompt distributions\n",
    "  - Model version changes\n",
    "  - Temperature/parameter differences\n",
    "  - Domain-specific behavior\n",
    "\n",
    "### Production Recommendations\n",
    "1. **High consistency + High performance**: Deploy with confidence\n",
    "2. **High consistency + Low performance**: Consider prompt engineering\n",
    "3. **Low consistency**: Investigate before production deployment\n",
    "4. **γ > 1.2**: Sequential pipelines are beneficial\n",
    "5. **γ < 1.0**: Avoid sequential processing, use single model or parallel architecture\n",
    "\n",
    "### What CERT Measures\n",
    "**Statistical Characterization:**\n",
    "- Behavioral variance in token generation (C)\n",
    "- Performance changes from context accumulation (γ)\n",
    "- Output quality distribution (μ, σ)\n",
    "\n",
    "**Engineering Decisions:**\n",
    "- Which pipeline configurations are reliable?\n",
    "- Does sequential processing help or hurt?\n",
    "- How predictable is production behavior?\n",
    "\n",
    "**NOT Measured:**\n",
    "- ❌ Agent intelligence or reasoning\n",
    "- ❌ Coordination principles or collaboration\n",
    "- ❌ Why context helps (attention mechanisms are black box)\n",
    "\n",
    "This is **engineering characterization** for deployment decisions, not coordination science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}