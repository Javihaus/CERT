{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Multi-Agent Pipeline: Research → Writer → Editor\n",
    "\n",
    "This notebook demonstrates a **real production multi-agent pipeline** using LangChain with CERT SDK instrumentation.\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "```\n",
    "User Query → Researcher Agent → Writer Agent → Editor Agent → Final Output\n",
    "```\n",
    "\n",
    "**Agents:**\n",
    "1. **Researcher**: Gathers information and key points\n",
    "2. **Writer**: Creates initial content draft\n",
    "3. **Editor**: Refines and polishes the final output\n",
    "\n",
    "**CERT Metrics Measured:**\n",
    "- Individual agent quality scores\n",
    "- Context propagation effect (γ) - performance changes from accumulated context\n",
    "- Pipeline health score\n",
    "- Execution timing and observability\n",
    "\n",
    "**Estimated time:** 3-5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install cert-sdk langchain langchain-openai langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cert\n",
    "from cert.integrations.langchain import CERTLangChain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize CERT Provider and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CERT provider for baseline comparison\n",
    "cert_provider = cert.create_provider(\n",
    "    api_key=api_key,\n",
    "    model_name=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# Get validated baseline\n",
    "baseline = cert.ModelRegistry.get_model(\"gpt-4o\")\n",
    "\n",
    "print(f\"✓ Using {baseline.model_id}\")\n",
    "print(f\"  Baseline: C={baseline.consistency:.3f}, μ={baseline.mean_performance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CERT LangChain integration\n",
    "cert_integration = CERTLangChain(\n",
    "    provider=cert_provider,\n",
    "    baseline=baseline,\n",
    "    verbose=True,  # Print execution details\n",
    ")\n",
    "\n",
    "print(\"✓ CERT LangChain integration initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Three Agents\n",
    "\n",
    "We'll create three specialized agents using **LangChain's LCEL (LangChain Expression Language)**.\n",
    "This is the modern, recommended approach - much simpler than custom classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LangChain LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key=api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(\"✓ LangChain LLM initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "# Uses LCEL pattern: prompt | llm\n",
    "researcher_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a research expert. Analyze the user's question and provide key research points, facts, and insights. Be thorough and factual.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Simple wrapper class to make LCEL chains work with CERT's message format\n",
    "class MessageAdapter:\n",
    "    \"\"\"Adapts LangChain LCEL chains to work with CERT's message format.\"\"\"\n",
    "    def __init__(self, chain):\n",
    "        self.chain = chain\n",
    "    \n",
    "    def invoke(self, input_data):\n",
    "        # Extract input from messages format\n",
    "        messages = input_data.get(\"messages\", [])\n",
    "        if messages:\n",
    "            last_msg = messages[-1]\n",
    "            user_input = last_msg.get(\"content\", \"\") if isinstance(last_msg, dict) else str(last_msg)\n",
    "        else:\n",
    "            user_input = input_data.get(\"input\", \"\")\n",
    "        \n",
    "        # Run the chain\n",
    "        response = self.chain.invoke({\"input\": user_input})\n",
    "        \n",
    "        # Return in messages format\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "                {\"role\": \"assistant\", \"content\": response.content}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Create researcher using LCEL\n",
    "researcher_chain = researcher_prompt | llm\n",
    "researcher = MessageAdapter(researcher_chain)\n",
    "\n",
    "print(\"✓ Researcher agent created (using LangChain LCEL)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 2: Writer\n",
    "writer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a professional writer. Take the research points provided and create a well-structured, engaging article. Focus on clarity and flow.\"),\n",
    "    (\"human\", \"Research points: {input}\"),\n",
    "])\n",
    "\n",
    "writer_chain = writer_prompt | llm\n",
    "writer = MessageAdapter(writer_chain)\n",
    "\n",
    "print(\"✓ Writer agent created (using LangChain LCEL)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 3: Editor\n",
    "editor_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert editor. Review the draft article and improve it by fixing grammar, enhancing clarity, and ensuring professional quality. Keep the core content but polish it.\"),\n",
    "    (\"human\", \"Draft to edit: {input}\"),\n",
    "])\n",
    "\n",
    "editor_chain = editor_prompt | llm\n",
    "editor = MessageAdapter(editor_chain)\n",
    "\n",
    "print(\"✓ Editor agent created (using LangChain LCEL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Instrumented Pipeline\n",
    "\n",
    "**Important:** `create_multi_agent_pipeline()` automatically wraps agents with CERT instrumentation.\n",
    "You don't need to call `wrap_agent()` separately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline - wrapping happens automatically inside this function\n",
    "pipeline = cert_integration.create_multi_agent_pipeline([\n",
    "    {\"agent\": researcher, \"agent_id\": \"researcher\", \"agent_name\": \"Research Agent\"},\n",
    "    {\"agent\": writer, \"agent_id\": \"writer\", \"agent_name\": \"Writer Agent\"},\n",
    "    {\"agent\": editor, \"agent_id\": \"editor\", \"agent_name\": \"Editor Agent\"},\n",
    "])\n",
    "\n",
    "print(\"✓ Multi-agent pipeline created with CERT instrumentation\")\n",
    "print(\"\\n  Pipeline: Research → Write → Edit → Final Output\")\n",
    "print(\"\\n  Note: All agents are automatically wrapped with CERT metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Let's test the pipeline with a real query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user query\n",
    "user_query = \"Explain the key factors in building successful multi-model LLM systems for production.\"\n",
    "\n",
    "print(f\"User Query: {user_query}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Executing Pipeline...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "result = pipeline({\"messages\": [{\"role\": \"user\", \"content\": user_query}]})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Pipeline Execution Complete\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final output\n",
    "final_output = result[\"messages\"][-1][\"content\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(\"=\"*70)\n",
    "print(final_output)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View CERT Metrics\n",
    "\n",
    "Now let's see the CERT metrics collected during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive metrics\n",
    "cert_integration.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the Results\n",
    "\n",
    "### Quality Scores\n",
    "- Each agent's output is scored for semantic relevance, linguistic coherence, and content density\n",
    "- Higher scores (closer to 1.0) indicate better quality\n",
    "\n",
    "### Context Propagation Effect (γ)\n",
    "**What it measures:**\n",
    "- Performance changes when models process accumulated context\n",
    "- How attention mechanisms handle extended context in sequential processing\n",
    "\n",
    "**What it does NOT measure:**\n",
    "- ❌ Agent coordination, collaboration, or planning\n",
    "- ❌ Intelligence or reasoning capabilities\n",
    "- ❌ WHY context helps (black box measurement)\n",
    "\n",
    "**Interpretation:**\n",
    "- **γ > 1.0**: Sequential context accumulation improves performance\n",
    "- **γ = 1.0**: No benefit from accumulated context\n",
    "- **γ < 1.0**: Context accumulation degrades performance\n",
    "\n",
    "### Pipeline Health\n",
    "- **H > 0.8**: Production ready - deploy with confidence\n",
    "- **0.6 < H < 0.8**: Acceptable - deploy with monitoring\n",
    "- **H < 0.6**: Needs investigation before production\n",
    "\n",
    "### Execution Timing\n",
    "- Shows duration for each agent\n",
    "- Helps identify bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Metrics Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access metrics as dictionary\n",
    "metrics_dict = cert_integration.get_metrics_summary()\n",
    "\n",
    "print(\"Metrics Summary:\")\n",
    "for key, value in metrics_dict.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Use Wrapped Agents Directly\n",
    "\n",
    "If you don't want to use `create_multi_agent_pipeline()`, you can wrap and execute agents manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Wrap agents manually\n",
    "# (Only run this if you didn't use create_multi_agent_pipeline above)\n",
    "\n",
    "# Reset metrics\n",
    "cert_integration.reset_metrics()\n",
    "\n",
    "# Wrap each agent individually\n",
    "instrumented_researcher = cert_integration.wrap_agent(\n",
    "    agent=researcher,\n",
    "    agent_id=\"researcher\",\n",
    "    agent_name=\"Research Agent\",\n",
    "    calculate_quality=True,\n",
    ")\n",
    "\n",
    "instrumented_writer = cert_integration.wrap_agent(\n",
    "    agent=writer,\n",
    "    agent_id=\"writer\",\n",
    "    agent_name=\"Writer Agent\",\n",
    "    calculate_quality=True,\n",
    ")\n",
    "\n",
    "instrumented_editor = cert_integration.wrap_agent(\n",
    "    agent=editor,\n",
    "    agent_id=\"editor\",\n",
    "    agent_name=\"Editor Agent\",\n",
    "    calculate_quality=True,\n",
    ")\n",
    "\n",
    "# Execute pipeline manually\n",
    "result = {\"messages\": [{\"role\": \"user\", \"content\": user_query}]}\n",
    "result = instrumented_researcher.invoke(result)\n",
    "result = instrumented_writer.invoke(result)\n",
    "result = instrumented_editor.invoke(result)\n",
    "\n",
    "# Calculate metrics manually\n",
    "cert_integration.calculate_coordination_effect()\n",
    "cert_integration.calculate_pipeline_health()\n",
    "\n",
    "print(\"✓ Manual pipeline execution complete\")\n",
    "cert_integration.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Queries\n",
    "\n",
    "Test the pipeline with different types of queries to see how context propagation effects vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries to try\n",
    "example_queries = [\n",
    "    \"What are the challenges in deploying LLM agents to production?\",\n",
    "    \"Explain how to measure AI agent performance and reliability.\",\n",
    "    \"Compare different multi-model frameworks for enterprise applications.\",\n",
    "]\n",
    "\n",
    "print(\"Try these queries:\")\n",
    "for i, query in enumerate(example_queries, 1):\n",
    "    print(f\"  {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Deployment\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Use CERT metrics** to validate your pipeline before production\n",
    "2. **Monitor context propagation effect (γ)** - if γ drops, investigate sequential processing behavior\n",
    "3. **Track health score** - set alerts if it falls below your threshold\n",
    "4. **Measure consistently** - run CERT measurements regularly to detect drift\n",
    "\n",
    "### What CERT Measures:\n",
    "\n",
    "✅ **Engineering Characterization:**\n",
    "- Statistical behavior of sequential LLM processing\n",
    "- Performance changes from context accumulation\n",
    "- Attention mechanism effects (black box measurement)\n",
    "- Operational metrics for deployment decisions\n",
    "\n",
    "❌ **What CERT Does NOT Measure:**\n",
    "- Agent coordination or collaboration\n",
    "- Intelligence or reasoning capabilities\n",
    "- WHY models improve with context\n",
    "\n",
    "This is **engineering instrumentation**, not coordination science.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try the CrewAI integration: `examples/crewai_pipeline.ipynb`\n",
    "- Learn about custom baselines: `examples/advanced_usage.py`\n",
    "- Explore the full API: See README.md for documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
