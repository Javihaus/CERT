{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT SDK - Advanced Features\n",
    "\n",
    "This notebook demonstrates advanced CERT SDK features:\n",
    "\n",
    "1. **Custom models** not in the validated registry\n",
    "2. **Domain-specific baselines** (Healthcare, Legal, Finance)\n",
    "3. **Custom quality scoring** with domain keywords\n",
    "4. **Baseline registration** for reuse\n",
    "\n",
    "**When to use this:**\n",
    "- Using models not in the validated registry (e.g., `gpt-4-turbo`, `llama-3`)\n",
    "- Building domain-specific agentic systems (Healthcare, Legal, Finance)\n",
    "- Need baselines tailored to your specific use case\n",
    "\n",
    "**Estimated time:** 5-10 minutes for full baseline measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CERT SDK\n",
    "# Option 1: From PyPI (when available)\n",
    "# !pip install cert-sdk\n",
    "\n",
    "# Option 2: Directly from GitHub repository (development version)\n",
    "# !pip install git+https://github.com/Javihaus/CERT.git\n",
    "\n",
    "import cert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using measure_custom_baseline()\n\nCERT SDK provides `cert.measure_custom_baseline()` for domain-specific applications. This function handles everything for you - no helper functions needed!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cert.measure_custom_baseline() function is now built into CERT SDK!\n",
    "# No need to define helper functions - just call it directly:\n",
    "#\n",
    "# consistency, mu, sigma = await cert.measure_custom_baseline(\n",
    "#     provider=provider,\n",
    "#     prompts=your_prompts,\n",
    "#     n_consistency_trials=20,\n",
    "#     domain_keywords=your_keywords,\n",
    "# )\n",
    "#\n",
    "# See example below for usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Healthcare Domain Custom Baseline\n",
    "\n",
    "This example shows how to measure baselines for healthcare-specific tasks with custom prompts and domain keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Healthcare-Specific Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_prompts = [\n",
    "    \"Analyze the key factors in patient care quality improvement.\",\n",
    "    \"Evaluate the main considerations for clinical decision support systems.\",\n",
    "    \"Assess the critical elements in healthcare data privacy and security.\",\n",
    "    \"Identify the primary aspects of telemedicine implementation.\",\n",
    "    \"Examine the essential components of medical staff coordination.\",\n",
    "    \"Analyze the challenges in healthcare resource allocation.\",\n",
    "    \"Evaluate diagnostic workflow optimization strategies.\",\n",
    "    \"Assess patient safety protocols and risk management.\",\n",
    "    \"Identify barriers to electronic health record adoption.\",\n",
    "    \"Examine factors in healthcare cost reduction.\",\n",
    "]\n",
    "\n",
    "print(f\"Healthcare prompts defined: {len(healthcare_prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Healthcare Domain Keywords\n",
    "\n",
    "These keywords will be used to score quality of responses - higher scores for responses that use domain-relevant terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_keywords = {\n",
    "    # Clinical terms\n",
    "    \"patient\",\n",
    "    \"clinical\",\n",
    "    \"diagnosis\",\n",
    "    \"treatment\",\n",
    "    \"care\",\n",
    "    \"medical\",\n",
    "    \"physician\",\n",
    "    \"nurse\",\n",
    "    \"provider\",\n",
    "    \"practitioner\",\n",
    "    # Healthcare operations\n",
    "    \"hospital\",\n",
    "    \"clinic\",\n",
    "    \"facility\",\n",
    "    \"healthcare\",\n",
    "    \"health\",\n",
    "    \"quality\",\n",
    "    \"safety\",\n",
    "    \"protocol\",\n",
    "    \"procedure\",\n",
    "    \"guideline\",\n",
    "    # Technology\n",
    "    \"ehr\",\n",
    "    \"emr\",\n",
    "    \"telemedicine\",\n",
    "    \"telehealth\",\n",
    "    \"digital\",\n",
    "    \"system\",\n",
    "    \"technology\",\n",
    "    \"data\",\n",
    "    \"record\",\n",
    "    \"information\",\n",
    "    # Management\n",
    "    \"workflow\",\n",
    "    \"process\",\n",
    "    \"management\",\n",
    "    \"coordination\",\n",
    "    \"efficiency\",\n",
    "    \"resource\",\n",
    "    \"allocation\",\n",
    "    \"optimization\",\n",
    "    \"improvement\",\n",
    "    # Compliance\n",
    "    \"hipaa\",\n",
    "    \"compliance\",\n",
    "    \"privacy\",\n",
    "    \"security\",\n",
    "    \"regulation\",\n",
    "    \"policy\",\n",
    "    \"standard\",\n",
    "    \"certification\",\n",
    "    \"accreditation\",\n",
    "}\n",
    "\n",
    "print(f\"Healthcare keywords defined: {len(healthcare_keywords)} keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model and Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using a model not in the registry\n",
    "model_id = \"gpt-4-turbo\"  # Not in validated registry\n",
    "\n",
    "# First, let's see what models are available\n",
    "print(\"Available validated models:\")\n",
    "cert.print_models(detailed=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "# Check if our model is validated\n",
    "if cert.ModelRegistry.is_validated(model_id):\n",
    "    print(f\"✓ {model_id} is in validated registry\")\n",
    "    cert.get_model_info(model_id)\n",
    "else:\n",
    "    print(f\"⚠ {model_id} is NOT in validated registry\")\n",
    "    print(\"  We need to measure a custom baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize provider - simple and direct!\n",
    "provider = cert.create_provider(\n",
    "    api_key=api_key,\n",
    "    model_name=model_id,\n",
    "    provider=\"openai\",  # Optional - auto-detected if not specified\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "print(f\"✓ Provider initialized: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Healthcare-Specific Baseline\n",
    "\n",
    "**Note:** This will take 5-10 minutes for full measurement with 20 consistency trials.\n",
    "\n",
    "For quick testing, you can reduce `n_consistency_trials` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline using built-in CERT SDK function - simple one-line call!\n",
    "consistency, mu, sigma = await cert.measure_custom_baseline(\n",
    "    provider=provider,\n",
    "    prompts=healthcare_prompts,\n",
    "    n_consistency_trials=20,  # Reduce to 10 for faster testing\n",
    "    domain_keywords=healthcare_keywords,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Custom Baseline\n",
    "\n",
    "Save your measured baseline in the registry for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_baseline = cert.ModelRegistry.register_custom_baseline(\n",
    "    model_id=model_id,\n",
    "    provider=\"openai\",\n",
    "    model_family=f\"{model_id} (Healthcare)\",\n",
    "    consistency=consistency,\n",
    "    mean_performance=mu,\n",
    "    std_performance=sigma,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Registered: {custom_baseline}\")\n",
    "print(\"\\nThis baseline is now available for use:\")\n",
    "print(f\"  cert.ModelRegistry.get_model('{model_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Paper Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to similar validated model if available\n",
    "gpt4o_baseline = cert.ModelRegistry.get_model(\"gpt-4o\")\n",
    "if gpt4o_baseline:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Comparison to Paper Baselines\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nGPT-4o (from paper - general analytical):\")\n",
    "    print(f\"  C = {gpt4o_baseline.consistency:.3f}\")\n",
    "    print(f\"  μ = {gpt4o_baseline.mean_performance:.3f}\")\n",
    "    print(f\"  σ = {gpt4o_baseline.std_performance:.3f}\")\n",
    "\n",
    "    print(f\"\\n{model_id} (Healthcare custom):\")\n",
    "    print(f\"  C = {consistency:.3f} ({consistency - gpt4o_baseline.consistency:+.3f})\")\n",
    "    print(f\"  μ = {mu:.3f} ({mu - gpt4o_baseline.mean_performance:+.3f})\")\n",
    "    print(f\"  σ = {sigma:.3f} ({sigma - gpt4o_baseline.std_performance:+.3f})\")\n",
    "\n",
    "    print(\"\\nNote: Differences expected due to:\")\n",
    "    print(\"  - Different model version\")\n",
    "    print(\"  - Different domain (Healthcare vs General Analytical)\")\n",
    "    print(\"  - Custom keyword scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Legal Domain\n",
    "\n",
    "Quick example showing Legal domain configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal-specific prompts\n",
    "legal_prompts = [\n",
    "    \"Analyze the key factors in contract negotiation strategy.\",\n",
    "    \"Evaluate risk management in corporate governance.\",\n",
    "    \"Assess compliance requirements for data protection.\",\n",
    "    \"Identify critical elements in intellectual property protection.\",\n",
    "    \"Examine due diligence processes in mergers and acquisitions.\",\n",
    "]\n",
    "\n",
    "# Legal-specific keywords\n",
    "legal_keywords = {\n",
    "    \"legal\",\n",
    "    \"law\",\n",
    "    \"regulation\",\n",
    "    \"compliance\",\n",
    "    \"contract\",\n",
    "    \"agreement\",\n",
    "    \"litigation\",\n",
    "    \"court\",\n",
    "    \"judge\",\n",
    "    \"attorney\",\n",
    "    \"liability\",\n",
    "    \"obligation\",\n",
    "    \"rights\",\n",
    "    \"statute\",\n",
    "    \"jurisdiction\",\n",
    "    \"evidence\",\n",
    "    \"precedent\",\n",
    "    \"case\",\n",
    "    \"ruling\",\n",
    "    \"counsel\",\n",
    "    \"due diligence\",\n",
    "    \"intellectual property\",\n",
    "    \"patent\",\n",
    "    \"copyright\",\n",
    "    \"governance\",\n",
    "    \"regulatory\",\n",
    "    \"policy\",\n",
    "    \"framework\",\n",
    "    \"standard\",\n",
    "}\n",
    "\n",
    "print(\"Legal Domain Configuration:\")\n",
    "print(f\"  - {len(legal_prompts)} legal-specific prompts\")\n",
    "print(f\"  - {len(legal_keywords)} legal keywords\")\n",
    "print(\"\\nTo measure:\")\n",
    "print(\"  consistency, mu, sigma = await measure_custom_baseline(\")\n",
    "print(\"      provider=your_provider,\")\n",
    "print(\"      prompts=legal_prompts,\")\n",
    "print(\"      n_consistency_trials=20,\")\n",
    "print(\"      domain_keywords=legal_keywords,\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Finance Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finance-specific prompts\n",
    "finance_prompts = [\n",
    "    \"Analyze key factors in portfolio risk management.\",\n",
    "    \"Evaluate strategies for market volatility assessment.\",\n",
    "    \"Assess critical elements in financial forecasting.\",\n",
    "    \"Identify primary aspects of investment diversification.\",\n",
    "    \"Examine components of credit risk evaluation.\",\n",
    "]\n",
    "\n",
    "# Finance-specific keywords\n",
    "finance_keywords = {\n",
    "    \"finance\",\n",
    "    \"financial\",\n",
    "    \"investment\",\n",
    "    \"portfolio\",\n",
    "    \"risk\",\n",
    "    \"return\",\n",
    "    \"capital\",\n",
    "    \"asset\",\n",
    "    \"liability\",\n",
    "    \"equity\",\n",
    "    \"market\",\n",
    "    \"trading\",\n",
    "    \"valuation\",\n",
    "    \"pricing\",\n",
    "    \"volatility\",\n",
    "    \"liquidity\",\n",
    "    \"credit\",\n",
    "    \"debt\",\n",
    "    \"bond\",\n",
    "    \"stock\",\n",
    "    \"diversification\",\n",
    "    \"hedge\",\n",
    "    \"derivative\",\n",
    "    \"option\",\n",
    "    \"futures\",\n",
    "    \"compliance\",\n",
    "    \"regulatory\",\n",
    "    \"audit\",\n",
    "    \"disclosure\",\n",
    "    \"reporting\",\n",
    "}\n",
    "\n",
    "print(\"Finance Domain Configuration:\")\n",
    "print(f\"  - {len(finance_prompts)} finance-specific prompts\")\n",
    "print(f\"  - {len(finance_keywords)} finance keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Quality Scoring Configuration\n",
    "\n",
    "You can also customize the quality scoring weights for different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quality Scoring Configuration Options\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nDefault (from paper - analytical tasks):\")\n",
    "print(\"  Semantic Relevance:    30%\")\n",
    "print(\"  Linguistic Coherence:  30%\")\n",
    "print(\"  Content Density:       40%\")\n",
    "print(\"  scorer = QualityScorer()\")\n",
    "\n",
    "print(\"\\nCreative writing:\")\n",
    "print(\"  Semantic Relevance:    20% (less critical)\")\n",
    "print(\"  Linguistic Coherence:  50% (very important)\")\n",
    "print(\"  Content Density:       30% (moderate)\")\n",
    "print(\"  scorer = QualityScorer(\")\n",
    "print(\"      semantic_weight=0.2,\")\n",
    "print(\"      coherence_weight=0.5,\")\n",
    "print(\"      density_weight=0.3,\")\n",
    "print(\"  )\")\n",
    "\n",
    "print(\"\\nTechnical documentation:\")\n",
    "print(\"  Semantic Relevance:    40% (very important)\")\n",
    "print(\"  Linguistic Coherence:  20% (less critical)\")\n",
    "print(\"  Content Density:       40% (very important)\")\n",
    "print(\"  scorer = QualityScorer(\")\n",
    "print(\"      semantic_weight=0.4,\")\n",
    "print(\"      coherence_weight=0.2,\")\n",
    "print(\"      density_weight=0.4,\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: When to Use Advanced Features\n",
    "\n",
    "### Use Custom Baselines When:\n",
    "1. **Model not in registry** - New models, fine-tuned models, or specific versions\n",
    "2. **Domain-specific tasks** - Healthcare, Legal, Finance require different evaluation\n",
    "3. **Custom prompts** - Your use case has unique prompt patterns\n",
    "4. **Quality criteria** - Default quality scoring doesn't match your needs\n",
    "\n",
    "### Best Practices:\n",
    "1. **Consistency trials:** Use 20+ for reliable measurements (paper standard)\n",
    "2. **Performance prompts:** Use 15+ diverse prompts (paper standard)\n",
    "3. **Domain keywords:** Include 30-50 relevant terms\n",
    "4. **Register baselines:** Save for reuse and team sharing\n",
    "5. **Compare to paper:** Use validated models as reference points\n",
    "\n",
    "### Production Workflow:\n",
    "1. Measure custom baseline during development\n",
    "2. Register in your model registry\n",
    "3. Use for ongoing monitoring and comparison\n",
    "4. Re-measure when model versions change\n",
    "5. Consider contributing validated baselines back to CERT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}