{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT SDK - Advanced Features\n",
    "\n",
    "This notebook demonstrates advanced CERT SDK features:\n",
    "\n",
    "1. **Custom models** not in the validated registry\n",
    "2. **Domain-specific baselines** (Healthcare, Legal, Finance)\n",
    "3. **Custom quality scoring** with domain keywords\n",
    "4. **Baseline registration** for reuse\n",
    "\n",
    "**When to use this:**\n",
    "- Using models not in the validated registry (e.g., `gpt-4-turbo`, `llama-3`)\n",
    "- Building domain-specific agentic systems (Healthcare, Legal, Finance)\n",
    "- Need baselines tailored to your specific use case\n",
    "\n",
    "**Estimated time:** 5-10 minutes for full baseline measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install CERT SDK if not already installed\n# !pip install cert-sdk\n\nimport asyncio\nimport cert\nfrom cert.models import ModelRegistry, ModelBaseline\nfrom cert.providers import OpenAIProvider, GoogleProvider\nfrom cert.providers.base import ProviderConfig\nfrom cert.analysis.semantic import SemanticAnalyzer\nfrom cert.analysis.quality import QualityScorer\nfrom cert.core.metrics import (\n    behavioral_consistency,\n    empirical_performance_distribution,\n)\nimport numpy as np"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Function: Measure Custom Baseline\n",
    "\n",
    "This function measures a complete baseline for any model with optional domain-specific customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def measure_custom_baseline(\n",
    "    provider,\n",
    "    prompts,\n",
    "    n_consistency_trials=20,\n",
    "    domain_keywords=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Measure complete custom baseline for any model.\n",
    "    \n",
    "    Args:\n",
    "        provider: Initialized provider with any model\n",
    "        prompts: List of domain-specific prompts for your use case\n",
    "        n_consistency_trials: Number of trials for consistency (20+ recommended)\n",
    "        domain_keywords: Optional set of domain-specific keywords for quality scoring\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (consistency, mean_performance, std_performance)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Custom Baseline Measurement\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nModel: {provider.config.model_name}\")\n",
    "    print(f\"Consistency trials: {n_consistency_trials}\")\n",
    "    print(f\"Performance prompts: {len(prompts)}\")\n",
    "    \n",
    "    # Step 1: Measure Behavioral Consistency\n",
    "    print(\"\\n[1/2] Measuring behavioral consistency...\")\n",
    "    print(f\"  Generating {n_consistency_trials} responses to same prompt...\")\n",
    "    \n",
    "    consistency_prompt = prompts[0]  # Use first prompt\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(n_consistency_trials):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=consistency_prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        responses.append(response)\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"    Progress: {i+1}/{n_consistency_trials}\")\n",
    "    \n",
    "    # Calculate consistency\n",
    "    analyzer = SemanticAnalyzer()\n",
    "    distances = analyzer.pairwise_distances(responses)\n",
    "    consistency = behavioral_consistency(distances)\n",
    "    \n",
    "    print(f\"  ✓ Behavioral Consistency: C = {consistency:.3f}\")\n",
    "    \n",
    "    # Step 2: Measure Performance Distribution\n",
    "    print(\"\\n[2/2] Measuring performance distribution...\")\n",
    "    print(f\"  Generating and scoring responses for {len(prompts)} prompts...\")\n",
    "    \n",
    "    # Create custom scorer if domain keywords provided\n",
    "    if domain_keywords:\n",
    "        scorer = QualityScorer(domain_keywords=domain_keywords)\n",
    "        print(f\"  Using {len(domain_keywords)} domain-specific keywords\")\n",
    "    else:\n",
    "        scorer = QualityScorer()\n",
    "        print(f\"  Using default analytical keywords\")\n",
    "    \n",
    "    quality_scores = []\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        response = await provider.generate_response(\n",
    "            prompt=prompt,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        \n",
    "        components = scorer.score(prompt, response)\n",
    "        quality_scores.append(components.composite_score)\n",
    "        \n",
    "        print(f\"    Prompt {i+1}/{len(prompts)}: Q = {components.composite_score:.3f}\")\n",
    "    \n",
    "    # Calculate distribution\n",
    "    mu, sigma = empirical_performance_distribution(np.array(quality_scores))\n",
    "    \n",
    "    print(f\"  ✓ Performance: μ = {mu:.3f}, σ = {sigma:.3f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Custom Baseline Results\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Consistency:   C = {consistency:.3f}\")\n",
    "    print(f\"Mean:          μ = {mu:.3f}\")\n",
    "    print(f\"Std Dev:       σ = {sigma:.3f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return consistency, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Healthcare Domain Custom Baseline\n",
    "\n",
    "This example shows how to measure baselines for healthcare-specific tasks with custom prompts and domain keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Healthcare-Specific Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_prompts = [\n",
    "    \"Analyze the key factors in patient care quality improvement.\",\n",
    "    \"Evaluate the main considerations for clinical decision support systems.\",\n",
    "    \"Assess the critical elements in healthcare data privacy and security.\",\n",
    "    \"Identify the primary aspects of telemedicine implementation.\",\n",
    "    \"Examine the essential components of medical staff coordination.\",\n",
    "    \"Analyze the challenges in healthcare resource allocation.\",\n",
    "    \"Evaluate diagnostic workflow optimization strategies.\",\n",
    "    \"Assess patient safety protocols and risk management.\",\n",
    "    \"Identify barriers to electronic health record adoption.\",\n",
    "    \"Examine factors in healthcare cost reduction.\",\n",
    "]\n",
    "\n",
    "print(f\"Healthcare prompts defined: {len(healthcare_prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Healthcare Domain Keywords\n",
    "\n",
    "These keywords will be used to score quality of responses - higher scores for responses that use domain-relevant terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthcare_keywords = {\n",
    "    # Clinical terms\n",
    "    \"patient\", \"clinical\", \"diagnosis\", \"treatment\", \"care\",\n",
    "    \"medical\", \"physician\", \"nurse\", \"provider\", \"practitioner\",\n",
    "    # Healthcare operations\n",
    "    \"hospital\", \"clinic\", \"facility\", \"healthcare\", \"health\",\n",
    "    \"quality\", \"safety\", \"protocol\", \"procedure\", \"guideline\",\n",
    "    # Technology\n",
    "    \"ehr\", \"emr\", \"telemedicine\", \"telehealth\", \"digital\",\n",
    "    \"system\", \"technology\", \"data\", \"record\", \"information\",\n",
    "    # Management\n",
    "    \"workflow\", \"process\", \"management\", \"coordination\", \"efficiency\",\n",
    "    \"resource\", \"allocation\", \"optimization\", \"improvement\",\n",
    "    # Compliance\n",
    "    \"hipaa\", \"compliance\", \"privacy\", \"security\", \"regulation\",\n",
    "    \"policy\", \"standard\", \"certification\", \"accreditation\",\n",
    "}\n",
    "\n",
    "print(f\"Healthcare keywords defined: {len(healthcare_keywords)} keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Model and Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Using a model not in the registry\nmodel_id = \"gpt-4-turbo\"  # Not in validated registry\n\n# First, let's see what models are available\nprint(\"Available validated models:\")\ncert.print_models(detailed=False)\n\nprint(\"\\n\" + \"=\"*70)\n# Check if our model is validated\nif ModelRegistry.is_validated(model_id):\n    print(f\"✓ {model_id} is in validated registry\")\n    cert.get_model_info(model_id)\nelse:\n    print(f\"⚠ {model_id} is NOT in validated registry\")\n    print(\"  We need to measure a custom baseline\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize provider\n",
    "config = ProviderConfig(\n",
    "    api_key=api_key,\n",
    "    model_name=model_id,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "provider = OpenAIProvider(config)\n",
    "print(f\"✓ Provider initialized: {provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Healthcare-Specific Baseline\n",
    "\n",
    "**Note:** This will take 5-10 minutes for full measurement with 20 consistency trials.\n",
    "\n",
    "For quick testing, you can reduce `n_consistency_trials` to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure baseline\n",
    "consistency, mu, sigma = await measure_custom_baseline(\n",
    "    provider=provider,\n",
    "    prompts=healthcare_prompts,\n",
    "    n_consistency_trials=20,  # Reduce to 10 for faster testing\n",
    "    domain_keywords=healthcare_keywords,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Custom Baseline\n",
    "\n",
    "Save your measured baseline in the registry for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_baseline = ModelRegistry.register_custom_baseline(\n",
    "    model_id=model_id,\n",
    "    provider=\"openai\",\n",
    "    model_family=f\"{model_id} (Healthcare)\",\n",
    "    consistency=consistency,\n",
    "    mean_performance=mu,\n",
    "    std_performance=sigma,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Registered: {custom_baseline}\")\n",
    "print(f\"\\nThis baseline is now available for use:\")\n",
    "print(f\"  ModelRegistry.get_model('{model_id}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Paper Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to similar validated model if available\n",
    "gpt4o_baseline = ModelRegistry.get_model(\"gpt-4o\")\n",
    "if gpt4o_baseline:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Comparison to Paper Baselines\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nGPT-4o (from paper - general analytical):\")\n",
    "    print(f\"  C = {gpt4o_baseline.consistency:.3f}\")\n",
    "    print(f\"  μ = {gpt4o_baseline.mean_performance:.3f}\")\n",
    "    print(f\"  σ = {gpt4o_baseline.std_performance:.3f}\")\n",
    "    \n",
    "    print(f\"\\n{model_id} (Healthcare custom):\")\n",
    "    print(f\"  C = {consistency:.3f} ({consistency - gpt4o_baseline.consistency:+.3f})\")\n",
    "    print(f\"  μ = {mu:.3f} ({mu - gpt4o_baseline.mean_performance:+.3f})\")\n",
    "    print(f\"  σ = {sigma:.3f} ({sigma - gpt4o_baseline.std_performance:+.3f})\")\n",
    "    \n",
    "    print(f\"\\nNote: Differences expected due to:\")\n",
    "    print(f\"  - Different model version\")\n",
    "    print(f\"  - Different domain (Healthcare vs General Analytical)\")\n",
    "    print(f\"  - Custom keyword scoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Legal Domain\n",
    "\n",
    "Quick example showing Legal domain configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal-specific prompts\n",
    "legal_prompts = [\n",
    "    \"Analyze the key factors in contract negotiation strategy.\",\n",
    "    \"Evaluate risk management in corporate governance.\",\n",
    "    \"Assess compliance requirements for data protection.\",\n",
    "    \"Identify critical elements in intellectual property protection.\",\n",
    "    \"Examine due diligence processes in mergers and acquisitions.\",\n",
    "]\n",
    "\n",
    "# Legal-specific keywords\n",
    "legal_keywords = {\n",
    "    \"legal\", \"law\", \"regulation\", \"compliance\", \"contract\",\n",
    "    \"agreement\", \"litigation\", \"court\", \"judge\", \"attorney\",\n",
    "    \"liability\", \"obligation\", \"rights\", \"statute\", \"jurisdiction\",\n",
    "    \"evidence\", \"precedent\", \"case\", \"ruling\", \"counsel\",\n",
    "    \"due diligence\", \"intellectual property\", \"patent\", \"copyright\",\n",
    "    \"governance\", \"regulatory\", \"policy\", \"framework\", \"standard\",\n",
    "}\n",
    "\n",
    "print(\"Legal Domain Configuration:\")\n",
    "print(f\"  - {len(legal_prompts)} legal-specific prompts\")\n",
    "print(f\"  - {len(legal_keywords)} legal keywords\")\n",
    "print(f\"\\nTo measure:\")\n",
    "print(f\"  consistency, mu, sigma = await measure_custom_baseline(\")\n",
    "print(f\"      provider=your_provider,\")\n",
    "print(f\"      prompts=legal_prompts,\")\n",
    "print(f\"      n_consistency_trials=20,\")\n",
    "print(f\"      domain_keywords=legal_keywords,\")\n",
    "print(f\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Finance Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finance-specific prompts\n",
    "finance_prompts = [\n",
    "    \"Analyze key factors in portfolio risk management.\",\n",
    "    \"Evaluate strategies for market volatility assessment.\",\n",
    "    \"Assess critical elements in financial forecasting.\",\n",
    "    \"Identify primary aspects of investment diversification.\",\n",
    "    \"Examine components of credit risk evaluation.\",\n",
    "]\n",
    "\n",
    "# Finance-specific keywords\n",
    "finance_keywords = {\n",
    "    \"finance\", \"financial\", \"investment\", \"portfolio\", \"risk\",\n",
    "    \"return\", \"capital\", \"asset\", \"liability\", \"equity\",\n",
    "    \"market\", \"trading\", \"valuation\", \"pricing\", \"volatility\",\n",
    "    \"liquidity\", \"credit\", \"debt\", \"bond\", \"stock\",\n",
    "    \"diversification\", \"hedge\", \"derivative\", \"option\", \"futures\",\n",
    "    \"compliance\", \"regulatory\", \"audit\", \"disclosure\", \"reporting\",\n",
    "}\n",
    "\n",
    "print(\"Finance Domain Configuration:\")\n",
    "print(f\"  - {len(finance_prompts)} finance-specific prompts\")\n",
    "print(f\"  - {len(finance_keywords)} finance keywords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Quality Scoring Configuration\n",
    "\n",
    "You can also customize the quality scoring weights for different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quality Scoring Configuration Options\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nDefault (from paper - analytical tasks):\")\n",
    "print(\"  Semantic Relevance:    30%\")\n",
    "print(\"  Linguistic Coherence:  30%\")\n",
    "print(\"  Content Density:       40%\")\n",
    "print(\"  scorer = QualityScorer()\")\n",
    "\n",
    "print(\"\\nCreative writing:\")\n",
    "print(\"  Semantic Relevance:    20% (less critical)\")\n",
    "print(\"  Linguistic Coherence:  50% (very important)\")\n",
    "print(\"  Content Density:       30% (moderate)\")\n",
    "print(\"  scorer = QualityScorer(\")\n",
    "print(\"      semantic_weight=0.2,\")\n",
    "print(\"      coherence_weight=0.5,\")\n",
    "print(\"      density_weight=0.3,\")\n",
    "print(\"  )\")\n",
    "\n",
    "print(\"\\nTechnical documentation:\")\n",
    "print(\"  Semantic Relevance:    40% (very important)\")\n",
    "print(\"  Linguistic Coherence:  20% (less critical)\")\n",
    "print(\"  Content Density:       40% (very important)\")\n",
    "print(\"  scorer = QualityScorer(\")\n",
    "print(\"      semantic_weight=0.4,\")\n",
    "print(\"      coherence_weight=0.2,\")\n",
    "print(\"      density_weight=0.4,\")\n",
    "print(\"  )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: When to Use Advanced Features\n",
    "\n",
    "### Use Custom Baselines When:\n",
    "1. **Model not in registry** - New models, fine-tuned models, or specific versions\n",
    "2. **Domain-specific tasks** - Healthcare, Legal, Finance require different evaluation\n",
    "3. **Custom prompts** - Your use case has unique prompt patterns\n",
    "4. **Quality criteria** - Default quality scoring doesn't match your needs\n",
    "\n",
    "### Best Practices:\n",
    "1. **Consistency trials:** Use 20+ for reliable measurements (paper standard)\n",
    "2. **Performance prompts:** Use 15+ diverse prompts (paper standard)\n",
    "3. **Domain keywords:** Include 30-50 relevant terms\n",
    "4. **Register baselines:** Save for reuse and team sharing\n",
    "5. **Compare to paper:** Use validated models as reference points\n",
    "\n",
    "### Production Workflow:\n",
    "1. Measure custom baseline during development\n",
    "2. Register in your model registry\n",
    "3. Use for ongoing monitoring and comparison\n",
    "4. Re-measure when model versions change\n",
    "5. Consider contributing validated baselines back to CERT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}